{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimension Reduction\n",
    "\n",
    "Nowadays, a lot of attributes are recorded for an individual. In order to fit a simple model, some sort of dimension reduction seems necessary. Mathematically dimension reduction is projection of data into a lower dimensional space. There are various reasons why dimension reduction is an interesting technique. For example :\n",
    "\n",
    "- Can be used for data visualization in high-dimensions.\n",
    "- The reduced dimensions can be used as new attributes for a better prediction.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-Visualization\n",
    "\n",
    "### Zip Code dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'data/'\n",
    "#filename = path + 'ziptrain.csv'\n",
    "zipdata = np.loadtxt(\"data/ziptrain.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7291, 257)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zipdata.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first column is the digit, the remaining 256 are pixes of a 16X16 grayscale image. Let's visualize the first row (image)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7ff979addeb8>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAD71JREFUeJzt3X+s1fV9x/HXy4tYFFdBh8UfmUIEdc02DTFoF1bGZOAvnPEPzESsNdgMHS6rSkOyNksg67p1m1tDw7xslBpoZnUao0MirYvJYEUGikUFlClIha0GupVgWd/743xp7r2ce7nn8/3BvX6ej+Tm/Pq+7+fN99wX3+/5nvM9H0eEAOTntFPdAIBTg/ADmSL8QKYIP5Apwg9kivADmSL8QKYIP5Apwg9kakSTg9nm44SnyKhRo5LqJk+enFS3b9++jmsOHjyYNBZ6iwgPZrlGw49TZ9KkSUl1GzZsSKpbsmRJxzXLly9PGgtp2O0HMlUq/LZn2X7T9i7bi6tqCkD9ksNvu0vSNyTNlnSlpDtsX1lVYwDqVWbLf42kXRHxdkR8JGmtpDnVtAWgbmXCf6Gk93rc3lvcB2AYKHO0v93bCSe8lWd7gaQFJcYBUIMy4d8r6eIety+S9H7fhSJihaQVEu/zA0NJmd3+H0i6zPaltkdKmivpmWraAlC35C1/RByzfb+kdZK6JK2MiNcr6wxArUp9wi8inpP0XEW9AGgQn/ADMkX4gUxxYs8wZA/qpK1eHnnkkaSxxowZk1Q3bdq0jms4sadZbPmBTBF+IFOEH8gU4QcyRfiBTBF+IFOEH8gU4QcyRfiBTBF+IFOEH8gU4Qcy5YjmvlmLr/Gqxo033thxzbPPPps0Vurfx7XXXttxzaZNm5LGQm+Dna6LLT+QKcIPZIrwA5kqM13Xxba/Z3uH7ddtL6qyMQD1KvNNPsck/XFEbLF9tqRXbK+PiB9W1BuAGiVv+SNif0RsKa7/RNIOMV0XMGxU8h1+ti+RdJWkE96rYbouYGgqHX7boyV9V9KDEXG47+NM1wUMTaWO9ts+Xa3gPx4RT1bTEoAmlDnab0ndknZExNerawlAE8ps+T8jaZ6k37a9tfi5oaK+ANSszESdL0vqfPYIAEMCn/ADMsVZfafQ5ZdfnlT30ksvdVwzbty4pLE2btyYVJdyVh+qwVl9AAZE+IFMEX4gU4QfyBThBzJF+IFMEX4gU4QfyBThBzJF+IFMEX4gU4QfyFQl3+GXuzPOOCOprru7O6ku5SSdvXv3Jo01b968pDoMfWz5gUwRfiBThB/IVOnw2+6y/R+20+aABnBKVLHlX6TWbD0AhpGy39t/kaQbJT1WTTsAmlJ2y//Xkh6W9PMKegHQoDKTdtwk6UBEvHKS5RbY3mx7c+pYAKpXdtKOW2zvkbRWrck7vt13oYhYERFTImJKibEAVKzMFN1fioiLIuISSXMlbYiIOyvrDECteJ8fyFQln+2PiO9L+n4VvwtAM9jyA5nirL4KLF26NKnuuuuuS6o7cuRIxzX33ntv0li7du1KqsPQx5YfyBThBzJF+IFMEX4gU4QfyBThBzJF+IFMEX4gU4QfyBThBzJF+IFMEX4gU4QfyBRn9fUxf/78jmseeOCBGjrp30MPPdRxzbp162ropFq2k+omTpzYcc2hQ4eSxjp48GBS3VDElh/IFOEHMlV20o5zbD9h+w3bO2xfW1VjAOpV9jX/30j6l4i43fZISWdW0BOABiSH3/YvSZom6W5JioiPJH1UTVsA6lZmt3+CpIOS/qGYpfcx22dV1BeAmpUJ/whJV0taHhFXSfpfSYv7LsR0XcDQVCb8eyXtjYhNxe0n1PrPoBem6wKGpjLTdf1I0nu2Jxd3zZD0w0q6AlC7skf7H5D0eHGk/21JnyvfEoAmlAp/RGyVxO48MAzxCT8gU46I5gazGxts/PjxSXVvvfVWxzWjR49OGmvNmjVJdXfddVfHNceOHUsaK9XUqVM7rlm2bFnSWNOnT++45sMPP0waq7u7O6ku5WSsVBExqDOk2PIDmSL8QKYIP5Apwg9kivADmSL8QKYIP5Apwg9kivADmSL8QKYIP5Apwg9kivADmfrYntW3evXqpLo777yz45rUqZ8mTZqUVHfgwIGOa1LPPEw9027hwoUd15x22tDfFqXmZcKECR3X7NmzJ2kszuoDMCDCD2Sq7HRdf2T7ddvbba+x/YmqGgNQr+Tw275Q0h9KmhIRn5bUJWluVY0BqFfZ3f4RkkbZHqHWPH3vl28JQBPKfG//Pkl/IeldSfslHYqIF6pqDEC9yuz2j5E0R9Klki6QdJbtE94nY7ouYGgqs9v/O5LeiYiDEfEzSU9Kuq7vQkzXBQxNZcL/rqSpts+0bbWm69pRTVsA6lbmNf8mtSbn3CLpteJ3raioLwA1Kztd15clfbmiXgA0iE/4AZki/ECmyk7R3YiRI0d2XDN79uwaOmlv3bp1SXUpZ+elWrVqVVLdbbfdllS3ffv2jmtWrlyZNNaRI0c6rlm+fHnSWKm6uroaHW8w2PIDmSL8QKYIP5Apwg9kivADmSL8QKYIP5Apwg9kivADmSL8QKYIP5Apwg9kalic2HPFFVd0XHPuuefW0El777zzTlLdqFGjkuqWLl3acc3NN9+cNNbatWuT6u67776Oaw4fPpw01sMPP5xUlyLlhCVJ2r17d8WdlMeWH8gU4QcyddLw215p+4Dt7T3uG2t7ve2dxeWYetsEULXBbPn/UdKsPvctlvRiRFwm6cXiNoBh5KThj4h/lfTjPnfPkXT8q2FWSbq14r4A1Cz1Nf/5EbFfkorLcdW1BKAJtb/VZ3uBpAV1jwOgM6lb/g9sj5ek4rLfb6Jkui5gaEoN/zOS5hfX50t6upp2ADRlMG/1rZH0b5Im295r+/OS/kzS9bZ3Srq+uA1gGDnpa/6IuKOfh2ZU3AuABvEJPyBThB/I1LA4q+/o0aOnuoUBzZ07N6lu4sSJSXW33357xzU7d+5MGuvuu+9OqmvyOZszZ05jYz399Mfn2DZbfiBThB/IFOEHMkX4gUwRfiBThB/IFOEHMkX4gUwRfiBThB/IFOEHMkX4gUw5IpobzE4abPTo0R3XvPnmmylD6YILLkiqG+pefvnlpLrU6aluvbXzL3QeMSLtPLOxY8d2XHPo0KGksaZPn55Ut23btqS6FBHhwSzHlh/IFOEHMkX4gUylztX3Ndtv2H7V9lO2z6m3TQBVS52rb72kT0fEr0l6S9KXKu4LQM2S5uqLiBci4lhxc6Oki2roDUCNqnjNf4+k5/t70PYC25ttb65gLAAVKfUFnraXSDom6fH+lomIFZJWFMs396ECAANKDr/t+ZJukjQjmvykEIBKJIXf9ixJj0j6rYj4abUtAWhC6lx9fyfpbEnrbW+1/c2a+wRQsdS5+rpr6AVAg/iEH5CpYXFWX4qZM2cm1a1evbrjmnHjxiWNhVPn0UcfTapbtGhRxZ1Uj7P6AAyI8AOZIvxApgg/kCnCD2SK8AOZIvxApgg/kCnCD2SK8AOZIvxApgg/kCnCD2TqY3tWX6qUudjWrFmTNNb555+fVIfeurs7/3qJhQsXJo119OjRpLomcVYfgAERfiBTSdN19Xjsi7bD9nn1tAegLqnTdcn2xZKul/RuxT0BaEDSdF2Fv5L0sKQhfxAPwIlSv7f/Fkn7ImKbPfCBRdsLJC1IGQdAfToOv+0zJS2RNKhvyGS6LmBoSjnaP1HSpZK22d6j1gy9W2x/qsrGANSr4y1/RLwm6RffVV38BzAlIv6rwr4A1Cx1ui4Aw1zqdF09H7+ksm4ANIZP+AGZ4sSeCnR1dSXVpU4pNm/evI5rZsyYkTRW6lRkGzZs6Lhm2bJljY3V5N990zixB8CACD+QKcIPZIrwA5ki/ECmCD+QKcIPZIrwA5ki/ECmCD+QKcIPZIrwA5ki/ECmmj6r76Ck/+zn4fMkDYVvA6KP3uijt6Hex69ExC8P5hc0Gv6B2N4cEVPogz7oo5k+2O0HMkX4gUwNpfCvONUNFOijN/ro7WPTx5B5zQ+gWUNpyw+gQY2G3/Ys22/a3mV7cZvHz7D9neLxTbYvqaGHi21/z/YO26/bXtRmmc/aPmR7a/HzJ1X30WOsPbZfK8bZ3OZx2360WCev2r664vEn9/h3brV92PaDfZapbX20mwLe9ljb623vLC7H9FM7v1hmp+35NfTxNdtvFOv9Kdvn9FM74HNYQR9fsb2vx/q/oZ/aAfN1goho5EdSl6TdkiZIGilpm6Qr+yzzB5K+WVyfK+k7NfQxXtLVxfWzJb3Vpo/PSnq2ofWyR9J5Azx+g6TnJVnSVEmban6OfqTWe8WNrA9J0yRdLWl7j/v+XNLi4vpiSV9tUzdW0tvF5Zji+piK+5gpaURx/avt+hjMc1hBH1+R9MVBPHcD5qvvT5Nb/msk7YqItyPiI0lrJc3ps8wcSauK609ImuGTTQPcoYjYHxFbius/kbRD0oVVjlGxOZK+FS0bJZ1je3xNY82QtDsi+vsgVuWi/RTwPf8OVkm6tU3p70paHxE/jogPJa2XNKvKPiLihYg4VtzcqNa8lLXqZ30MxmDy1UuT4b9Q0ns9bu/ViaH7xTLFSj8k6dy6GipeVlwlaVObh6+1vc3287Z/ta4eJIWkF2y/Ukxn3tdg1ltV5kpa089jTa0PSTo/IvZLrf+s1WNuyB6aXC+SdI9ae2DtnOw5rML9xcuPlf28DOp4fTQZ/nZb8L5vNQxmmUrYHi3pu5IejIjDfR7eotau769L+ltJ/1xHD4XPRMTVkmZLWmh7Wt9W29RUvk5sj5R0i6R/avNwk+tjsJr8W1ki6Zikx/tZ5GTPYVnL1Zod+zck7Zf0l+3abHPfgOujyfDvlXRxj9sXSXq/v2Vsj5D0SaXtAg3I9ulqBf/xiHiy7+MRcTgi/qe4/pyk022fV3Ufxe9/v7g8IOkptXbfehrMeqvCbElbIuKDNj02tj4KHxx/aVNcHmizTCPrpTiQeJOk34/ixXVfg3gOS4mIDyLi/yLi55L+vp/f3/H6aDL8P5B0me1Li63MXEnP9FnmGUnHj9reLmlDfys8VXEMoVvSjoj4ej/LfOr4sQbb16i1nv67yj6K332W7bOPX1frANP2Pos9I+mu4qj/VEmHju8SV+wO9bPL39T66KHn38F8SU+3WWadpJm2xxS7wTOL+ypje5akRyTdEhE/7WeZwTyHZfvoeYzn9/r5/YPJV29VHKHs4EjmDWodXd8taUlx35+qtXIl6RNq7XbukvTvkibU0MNvqrU79KqkrcXPDZK+IOkLxTL3S3pdrSOmGyVdV9P6mFCMsa0Y7/g66dmLJX2jWGevSZpSQx9nqhXmT/a4r5H1odZ/OPsl/Uytrdfn1TrO86KkncXl2GLZKZIe61F7T/G3skvS52roY5dar6OP/50cfyfqAknPDfQcVtzH6uK5f1WtQI/v20d/+Rroh0/4AZniE35Apgg/kCnCD2SK8AOZIvxApgg/kCnCD2SK8AOZ+n9unHS4R7GidQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(zipdata[0, 1:].reshape(16,16), \"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By taking the opposite values, we get the opposite gray scale representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(-zipdata[0, 1:].reshape(16,16), \"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if the associated label is a 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zipdata[0,0] == 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the second image also.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(-zipdata[1, 1:].reshape(16,16), \"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zipdata[1,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and it is a 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important that you feel comfortable with numpy arrays: concatenating, deleting, reshaping, etc. Here I provide you some examples. Let's select images that only contain value 3, and visualize a random sample of such images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zipdata3 = zipdata[zipdata[:, 0] == 3]\n",
    "zipdata3.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only 658 samples are 3. Let's visualize a random sample of them of size 20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "random3 = np.random.choice(range(len(zipdata3)), size=20, replace = False)\n",
    "\n",
    "print(random3)\n",
    "#gives you the index of the selected random images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zipdata3stack = zipdata3[random3[0], 1:].reshape(16, 16)\n",
    "print(zipdata3stack)\n",
    "plt.imshow(-zipdata3stack,\"gray\");\n",
    "plt.axis(\"off\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,len(random3)):\n",
    "    zipdata3stack = np.hstack((zipdata3stack, zipdata3[random3[i],1:].reshape(16, 16)))\n",
    "\n",
    "plt.imshow(-zipdata3stack,\"gray\");\n",
    "plt.axis(\"off\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You may want to plot vertically\n",
    "# This is a more clever coding, we do not need  \"for\" loop\n",
    "plt.imshow(-zipdata3[random3,1:].reshape(320,16),\"gray\");\n",
    "plt.axis(\"off\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subset of the dataset: digits 3 and 8\n",
    "\n",
    "Let's create a classification problem. The goal is to classify 3 and 8 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zipdata8 = zipdata[zipdata[:, 0] == 8]\n",
    "zipdata8.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zipdata38 = np.vstack([zipdata3, zipdata8])\n",
    "zipdata38.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "\n",
    "# remove the first column (image label)\n",
    "pca.fit(zipdata38[:, 1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = pca.transform(zipdata38[:,1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(Z[:,0], Z[:,1], c= zipdata38[:,0], alpha=0.3);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This representation is much more clear than what we can do with the original 256 pixels (covariates)\n",
    "#plt.scatter(zipdata38[:,100], zipdata38[:,200], c= zipdata38[:,0], alpha=0.3);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can make it a little bit more visual with the following lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(Z[zipdata38[:,0]==3,0], Z[zipdata38[:,0]==3,1], marker='$3$', \n",
    "            color='blue', alpha = 0.3);\n",
    "\n",
    "plt.scatter(Z[zipdata38[:,0]==8,0], Z[zipdata38[:,0]==8,1], marker='$8$', \n",
    "            color='red', alpha = 0.3);\n",
    "\n",
    "plt.xlim([-10,10])\n",
    "plt.ylim([-10,10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary Classification on PC1 and PC2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lr = LinearRegression()\n",
    "predictors = Z\n",
    "outcome = (zipdata38[:,0] > 4)*1\n",
    "lr.fit(predictors, outcome)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lr.intercept_, lr.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(Z[zipdata38[:,0]==3,0], Z[zipdata38[:,0]==3,1], marker='$3$', \n",
    "            color='blue', alpha = 0.3);\n",
    "\n",
    "plt.scatter(Z[zipdata38[:,0]==8,0], Z[zipdata38[:,0]==8,1], marker='$8$', \n",
    "            color='red', alpha = 0.3);\n",
    "\n",
    "pred = np.linspace(start = -10, stop = 10, num= 100)\n",
    "plt.plot(pred, -(lr.coef_[0]/lr.coef_[1]) * pred, \"-k\")\n",
    "\n",
    "plt.xlim([-10,10])\n",
    "plt.ylim([-10,10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auto dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path='data/'\n",
    "filename = path+'Auto.csv'\n",
    "auto = pd.read_csv(filename, na_values=['?'], na_filter=True)\n",
    "auto = auto.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only quantitative variables\n",
    "X = auto[['cylinders', 'displacement', 'horsepower', 'weight', 'acceleration']]\n",
    "y = auto['mpg']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, fit a PCA on original values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(X.values)\n",
    "Z = pca.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(Z[:,0], Z[:,1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, work with standardized values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import scale\n",
    "X_std = scale(X.values)\n",
    "pca_std = PCA(n_components=2)\n",
    "pca_std.fit(X_std)\n",
    "Z_std = pca_std.transform(X_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(Z_std[:,0], Z_std[:,1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now Z includes a summary of all variables in X only in two dimensions. Further more, principal components of the data matrix and the standardized data matrix are different."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-Principal Component Regression\n",
    "We may use principal components as attributes to feed a linear regression. Often it provides a better prediction in low dimensions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lr = LinearRegression()\n",
    "X_simple = X[['horsepower']]\n",
    "lr.fit(X_simple,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.score(X_simple, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcr = LinearRegression()\n",
    "Z_simple = Z[:,0].reshape(-1,1)\n",
    "pcr.fit(Z_simple, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcr.score(Z_simple,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the R squared increased when using the first principal component instead of horsepower variable for fitting a simple linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The PCA we defined in the above code was arbitrarely a projection onto a space of dimension 2. In practice, we will choose the best projection dimension $M$ with cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "X_reduced = pca.fit_transform(scale(X))\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_reduced.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(X_reduced).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10-fold CV, with shuffle\n",
    "n = len(X_reduced)\n",
    "kf_10 = model_selection.KFold(n_splits=10, shuffle=True)\n",
    "\n",
    "regr = LinearRegression()\n",
    "r2 = []\n",
    "\n",
    "# Calculate MSE with only the intercept (no principal components in regression)\n",
    "score = model_selection.cross_val_score(regr, np.ones((n,1)), y.ravel(), cv=kf_10, \\\n",
    "                                           scoring='r2').mean()    \n",
    "r2.append(score)\n",
    "\n",
    "# Calculate MSE using CV for the 5 principle components, adding one component at the time.\n",
    "for i in np.arange(1, 6):\n",
    "    score = model_selection.cross_val_score(regr, X_reduced[:,:i], y.ravel(), cv=kf_10, \\\n",
    "                                               scoring='r2').mean()\n",
    "    r2.append(score)\n",
    "    \n",
    "# Plot results    \n",
    "plt.plot(r2, '-v')\n",
    "plt.xlabel('Number of principal components in regression')\n",
    "plt.ylabel('R Squared')\n",
    "plt.title('mpg');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the largest cross-validation $R^2$ occurs when  $M=5$  components are used, which amounts to simply performing least squares, because when all of the components are used in PCR no dimension reduction occurs. However, from the plot we also see that the cross-validation error is roughly the same when only one component is included in the model. This suggests that a model that uses just a small number of components might suffice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We'll do a little math to get the amount of variance explained by adding each consecutive principal component:\n",
    "\n",
    "np.cumsum(np.round(pca.explained_variance_ratio_, decimals=2)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll dig deeper into this concept later, but for now we can think of this as the amount of information about the predictors or the response that is captured using  $M$ principal components. For example, setting  $M=1$ captures $81\\%$ of all the variance, or information, in the predictors. In contrast, using  $M=2$  increases the value to $95\\%$. If we were to use all  $M=p=5$  components, this would increase to 100%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction\n",
    "\n",
    "Now let's perform PCA on the training data and evaluate its test set performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca2 = PCA()\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test , y_train, y_test = model_selection.train_test_split(X, y, test_size=0.5, random_state=1)\n",
    "\n",
    "# Scale the data\n",
    "X_reduced_train = pca2.fit_transform(scale(X_train))\n",
    "n = len(X_reduced_train)\n",
    "\n",
    "# 10-fold CV, with shuffle\n",
    "kf_10 = model_selection.KFold(n_splits=10, shuffle=True, random_state=1)\n",
    "\n",
    "r2 = []\n",
    "\n",
    "# Calculate R2 with only the intercept (no principal components in regression)\n",
    "score = model_selection.cross_val_score(regr, np.ones((n,1)), y_train.ravel(), cv=kf_10, \\\n",
    "                                           scoring='r2').mean()    \n",
    "r2.append(score)\n",
    "\n",
    "# Calculate R2 using CV for the 5 principal components, adding one component at the time.\n",
    "for i in np.arange(1, 6):\n",
    "    score = model_selection.cross_val_score(regr, X_reduced_train[:,:i], y_train.ravel(), cv=kf_10, \\\n",
    "                                             scoring='r2').mean()\n",
    "    r2.append(score)\n",
    "\n",
    "    \n",
    "plt.plot(np.array(r2), '-v')\n",
    "plt.xlabel('Number of principal components in regression')\n",
    "plt.ylabel('R Squared')\n",
    "plt.title('mpg');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find that the highest cross-validation $R^2$ occurs when  $M=3$ components are used. Now we'll see how it performs on the test data and compute the test $R^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M=3\n",
    "X_reduced_test = pca2.transform(scale(X_test))[:,:M]\n",
    "\n",
    "# Train regression model on training data \n",
    "regr = LinearRegression()\n",
    "regr.fit(X_reduced_train[:,:M], y_train)\n",
    "\n",
    "# R squared on test data\n",
    "print(\"R Squared:\", regr.score(X_reduced_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "pred = regr.predict(X_reduced_test)\n",
    "print(\"MSE:\", mean_squared_error(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison with Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a Ridge regression with arbitrary regularization constant, take 0.005 for example\n",
    "rr = Ridge(alpha=0.005, normalize=True)\n",
    "rr.fit(X_train, y_train) \n",
    "\n",
    "# Predict on the test set and compute MSE\n",
    "rr_pred = rr.predict(X_test)\n",
    "print(\"R Squared:\", rr.score(X_test, y_test))\n",
    "print(\"MSE:\", mean_squared_error(y_test, rr_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test set MSE for PCR is competitive with the results obtained using ridge regression. However, as a result of the way PCR is implemented, the final model is more difficult to interpret because it does not perform any kind of variable selection or even directly produce coefficient estimates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-Partial Least Squares\n",
    "It makes sense if we want to project X matrix for predicting some y, to make the projection function of y. This can be regarded as the right projection method for linear regression. The PLS projection is a slightly different version of PCA: the algorithm takes account for y while projecting data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_decomposition import PLSRegression\n",
    "pls = PLSRegression(n_components=2)\n",
    "pls.fit(X, y)\n",
    "W = pls.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(W[:,0], W[:,1]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pls.score(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(X_train)\n",
    "\n",
    "# 10-fold CV, with shuffle\n",
    "kf_10 = model_selection.KFold(n_splits=10, shuffle=True, random_state=1)\n",
    "\n",
    "r2 = []\n",
    "\n",
    "for i in np.arange(1, 6):\n",
    "    pls = PLSRegression(n_components=i)\n",
    "    score = model_selection.cross_val_score(pls, scale(X_train), y_train, cv=kf_10, \\\n",
    "                                            scoring='r2').mean()\n",
    "    r2.append(score)\n",
    "\n",
    "# Plot results\n",
    "plt.plot(np.arange(1, 6), np.array(r2), '-v')\n",
    "plt.xlabel('Number of principal components in regression')\n",
    "plt.ylabel('R Squared')\n",
    "plt.title('mpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's compute the R Squared and MSE on the test set for Partial Least Squares with M=3 components.\n",
    "\n",
    "pls = PLSRegression(n_components=3)\n",
    "pls.fit(scale(X_train), y_train)\n",
    "\n",
    "print(\"R Squared:\", pls.score(scale(X_test),y_test))\n",
    "print(\"MSE:\", mean_squared_error(y_test, pls.predict(scale(X_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"R Squared for PCR with M=3:\", round(regr.score(X_reduced_test, y_test),3))\n",
    "print(\"R Squared for PLS with M=3:\", round(pls.score(scale(X_test),y_test),3))\n",
    "print(\"R Squared for Ridge:\", round(rr.score(X_test, y_test),3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
